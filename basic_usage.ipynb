{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 认识Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们可以从huggingface hub中直接加载一个训练好的tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载出来的Tokenizer是一个`BertTokenizerFast`类型的对象，里面包含了：`vocab_size`，`special_tokens`，padding的控制等信息。\n",
    "\n",
    "special_tokens_map中记录了模型使用的一些特殊的token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "[100, 102, 0, 101, 103]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer.convert_tokens_to_ids(tokenizer.special_tokens_map.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 基本用法\n",
    "\n",
    "## 2.1 直接调用\n",
    "\n",
    "直接调用`__call__`方法，可以直接对句子进行Tokenization。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\"today is not so bad\", \"It is so bad\", \"It's good\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "tensor([[ 101, 2651, 2003, 2025, 2061, 2919,  102],\n",
      "        [ 101, 2009, 2003, 2061, 2919,  102,    0],\n",
      "        [ 101, 2009, 1005, 1055, 2204,  102,    0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "in_tensors = tokenizer(\n",
    "    test_examples,\n",
    "    padding=\"longest\",\n",
    "    truncation=True,\n",
    "    max_length=32,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(in_tensors.keys())\n",
    "print(in_tensors[\"input_ids\"])\n",
    "print(in_tensors[\"attention_mask\"])\n",
    "print(in_tensors[\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__call__`方法返回的是一个`BatchEncoding`对象，它是`dict`的子类，所以我们可以通过`[key]`来索引。\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/tokenizer#transformers.BatchEncoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. 填充与截段\n",
    "\n",
    "我们在上面进行分词时，设置了`padding=\"longest\"`的选项，说明了，所以句子都按batch中最长的句子的长度进行对齐，如果长度不足，则补充`padding token`，padding可以有以下几种选项：\n",
    "\n",
    "* `True`或者`longest`：填充到批次中最长的序列（如果只提供一个序列，则不应用填充）。\n",
    "* `max_length`: 填充到`max_length`参数指定的长度，如果没有提供`max_length`参数，则填充到模型接受的最大长度（`model_max_length`）。如果您只提供了一个序列，则仍将应用填充。\n",
    "* `False`或者`do_not_pad`：不应用填充。这是默认行为。\n",
    "\n",
    "同时我们也在上面设置了截段的选项`truncation=True`，这个参数的可能的选项有：\n",
    "\n",
    "* `True`或者`longest_first`：截断到`max_length`指定的最大长度，如果如果没有提供`max_length`参数，则截断到模型接受的最大长度（`model_max_length`）。如果传入的是一个句子对，那么它将从最长的那个句子中删除字符，直到长度满足为止。如果在删除的过程中，会不断的检查剩余的两个句子的长度，选项最长的那个句子来删除token。\n",
    "* `only_second`: 对于单个句子截断的行为与`True`一致。如果提供的是一对序列（或一批成对的序列），这只会截断一对序列的第二句，如果第二个句子不够截取，则报错。\n",
    "* `only_first`: 对于单个句子截断的行为与`True`一致。如果提供的是一对序列（或一批成对的序列），这只会截断一对序列的第一句，如果第一个句子不够截取，则报错。\n",
    "* `False`或者`do_not_truncate`：不进行截断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. 分词：tokenize：将句子转换为token（word piece）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today', 'is', 'not', 'so', 'bad']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不支持batch调用\n",
    "tokenizer.tokenize(test_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[UNK]', '，', '中', '国', '，', 'bert', '[UNK]', '中', '文', '的', '[UNK]', '[UNK]', '[UNK]', '有', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "# 我们使用的Bert模型对中文支持有限，不在vocab中的中文会被转换为 [UNK]\n",
    "print(tokenizer.tokenize(\"你好，中国，Bert对中文的支持很有限\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '-', 'cat', '!', ',', 'hugging', '##face', ',', '123', '##45', '##6']\n"
     ]
    }
   ],
   "source": [
    "# 对于一些生僻，错误的word，会进行拆为wordpiece\n",
    "print(tokenizer.tokenize(\"hello-cat!, huggingFace, 123456\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '-', 'cat', '!', ',', 'hugging', '##face', ',', '123', '##45', '##6']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"hello-cat!, huggingFace, 123456\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. token和id的相互转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2651, 2003, 2025, 2061, 2919]\n",
      "['today', 'is', 'not', 'so', 'bad']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(test_examples[0])\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)\n",
    "print(tokenizer.convert_ids_to_tokens(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. `encode`\n",
    "\n",
    "encode接口只能返回 token ids，而且不支持batch调用。等价于：`convert_token_to_id(tokenize(text))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2651, 2003, 2025, 2061, 2919, 102, 2009, 2003, 2061, 2919, 102]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\n",
    "    test_examples[0],\n",
    "    test_examples[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'today', 'is', 'not', 'so', 'bad', '[SEP]', 'it', 'is', 'so', 'bad', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tokenizer.convert_ids_to_tokens(\n",
    "        tokenizer.encode(test_examples[0], test_examples[1])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. `encode_plus`\n",
    "\n",
    "该接口已经deprecated，完全被`__call__`替换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. decode\n",
    "\n",
    "decode是encode的逆运算：将id list 转化为一个字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] today is not so bad [SEP] it is so bad [SEP]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(test_examples[0], test_examples[1])\n",
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fast Tokenizer / Slow Tokenizer\n",
    "\n",
    "Fast Tokenizer 是基于rust来实现的，速度快；而Slow tokenizer是基于python实现，速度慢；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "slow_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_long_texts = [\n",
    "    \"A tokenizer is in charge of preparing the inputs for a model. The library contains tokenizers for all the models. Most of the tokenizers are available in two flavors: a full python implementation and a “Fast” implementation based on the Rust library 🤗 Tokenizers. The “Fast” implementations allows:\",\n",
    "    \"The base classes PreTrainedTokenizer and PreTrainedTokenizerFast implement the common methods for encoding string inputs in model inputs (see below) and instantiating/saving python and “Fast” tokenizers either from a local file or directory or from a pretrained tokenizer provided by the library (downloaded from HuggingFace’s AWS S3 repository). They both rely on PreTrainedTokenizerBase that contains the common methods, and SpecialTokensMixin.\",\n",
    "    \"BatchEncoding holds the output of the PreTrainedTokenizerBase’s encoding methods (__call__, encode_plus and batch_encode_plus) and is derived from a Python dictionary. When the tokenizer is a pure python tokenizer, this class behaves just like a standard python dictionary and holds the various model inputs computed by these methods (input_ids, attention_mask…). When the tokenizer is a “Fast” tokenizer (i.e., backed by HuggingFace tokenizers library), this class provides in addition several advanced alignment methods which can be used to map between the original string (character and words) and the token space (e.g., getting the index of the token comprising a given character or the span of characters corresponding to a given token).\",\n",
    "    \"Handle all the shared methods for tokenization and special tokens as well as methods downloading/caching/loading pretrained tokenizers as well as adding tokens to the vocabulary.\",\n",
    "]\n",
    "# bathc size = 4 * 8\n",
    "batch_long_texts = batch_long_texts * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.94 ms ± 9.04 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "fast_tokenizer(batch_long_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 ms ± 446 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "slow_tokenizer(batch_long_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的对比，可以看出在批量模型式下，FastTokenizer是SlowTokenizer的20倍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. `offset_mapping`和`word_ids`\n",
    "FastTokenizer有一些特殊的返回值\n",
    "\n",
    "* offset_mapping：标记了每一个token在原输出str中字符级别的索引位置\n",
    "* word_ids：标记了每个token对应原输出中word的索引\n",
    "\n",
    "这个对于NER或QA来说比较重要。\n",
    "\n",
    "<div align=\"left\">\n",
    "  <img src=\"./assets/tokenizer.drawio.svg\" width=\"660\"/> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1999, 1996, 2502, 2502, 2088, 1010, 1045, 2031, 1037, 2502, 3959, 6562, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 2), (3, 6), (7, 10), (11, 14), (15, 20), (20, 21), (22, 23), (24, 28), (29, 30), (31, 34), (35, 40), (40, 44), (0, 0)]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = fast_tokenizer(\n",
    "    \"In the big big world, I have a big dreamming\", return_offsets_mapping=True\n",
    ")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 2), (3, 6), (7, 10), (11, 14), (15, 20), (20, 21), (22, 23), (24, 28), (29, 30), (31, 34), (35, 40), (40, 44), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "print(inputs.offset_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, None]\n"
     ]
    }
   ],
   "source": [
    "print(inputs.word_ids())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
