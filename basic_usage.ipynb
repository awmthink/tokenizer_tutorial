{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. è®¤è¯†Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æˆ‘ä»¬å¯ä»¥ä»huggingface hubä¸­ç›´æ¥åŠ è½½ä¸€ä¸ªè®­ç»ƒå¥½çš„tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŠ è½½å‡ºæ¥çš„Tokenizeræ˜¯ä¸€ä¸ª`BertTokenizerFast`ç±»å‹çš„å¯¹è±¡ï¼Œé‡Œé¢åŒ…å«äº†ï¼š`vocab_size`ï¼Œ`special_tokens`ï¼Œpaddingçš„æ§åˆ¶ç­‰ä¿¡æ¯ã€‚\n",
    "\n",
    "special_tokens_mapä¸­è®°å½•äº†æ¨¡å‹ä½¿ç”¨çš„ä¸€äº›ç‰¹æ®Šçš„tokenã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "[100, 102, 0, 101, 103]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer.convert_tokens_to_ids(tokenizer.special_tokens_map.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. åŸºæœ¬ç”¨æ³•\n",
    "\n",
    "## 2.1 ç›´æ¥è°ƒç”¨\n",
    "\n",
    "ç›´æ¥è°ƒç”¨`__call__`æ–¹æ³•ï¼Œå¯ä»¥ç›´æ¥å¯¹å¥å­è¿›è¡ŒTokenizationã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\"today is not so bad\", \"It is so bad\", \"It's good\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "tensor([[ 101, 2651, 2003, 2025, 2061, 2919,  102],\n",
      "        [ 101, 2009, 2003, 2061, 2919,  102,    0],\n",
      "        [ 101, 2009, 1005, 1055, 2204,  102,    0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "in_tensors = tokenizer(\n",
    "    test_examples,\n",
    "    padding=\"longest\",\n",
    "    truncation=True,\n",
    "    max_length=32,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(in_tensors.keys())\n",
    "print(in_tensors[\"input_ids\"])\n",
    "print(in_tensors[\"attention_mask\"])\n",
    "print(in_tensors[\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__call__`æ–¹æ³•è¿”å›çš„æ˜¯ä¸€ä¸ª`BatchEncoding`å¯¹è±¡ï¼Œå®ƒæ˜¯`dict`çš„å­ç±»ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥é€šè¿‡`[key]`æ¥ç´¢å¼•ã€‚\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/tokenizer#transformers.BatchEncoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. å¡«å……ä¸æˆªæ®µ\n",
    "\n",
    "æˆ‘ä»¬åœ¨ä¸Šé¢è¿›è¡Œåˆ†è¯æ—¶ï¼Œè®¾ç½®äº†`padding=\"longest\"`çš„é€‰é¡¹ï¼Œè¯´æ˜äº†ï¼Œæ‰€ä»¥å¥å­éƒ½æŒ‰batchä¸­æœ€é•¿çš„å¥å­çš„é•¿åº¦è¿›è¡Œå¯¹é½ï¼Œå¦‚æœé•¿åº¦ä¸è¶³ï¼Œåˆ™è¡¥å……`padding token`ï¼Œpaddingå¯ä»¥æœ‰ä»¥ä¸‹å‡ ç§é€‰é¡¹ï¼š\n",
    "\n",
    "* `True`æˆ–è€…`longest`ï¼šå¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›ä¸€ä¸ªåºåˆ—ï¼Œåˆ™ä¸åº”ç”¨å¡«å……ï¼‰ã€‚\n",
    "* `max_length`: å¡«å……åˆ°`max_length`å‚æ•°æŒ‡å®šçš„é•¿åº¦ï¼Œå¦‚æœæ²¡æœ‰æä¾›`max_length`å‚æ•°ï¼Œåˆ™å¡«å……åˆ°æ¨¡å‹æ¥å—çš„æœ€å¤§é•¿åº¦ï¼ˆ`model_max_length`ï¼‰ã€‚å¦‚æœæ‚¨åªæä¾›äº†ä¸€ä¸ªåºåˆ—ï¼Œåˆ™ä»å°†åº”ç”¨å¡«å……ã€‚\n",
    "* `False`æˆ–è€…`do_not_pad`ï¼šä¸åº”ç”¨å¡«å……ã€‚è¿™æ˜¯é»˜è®¤è¡Œä¸ºã€‚\n",
    "\n",
    "åŒæ—¶æˆ‘ä»¬ä¹Ÿåœ¨ä¸Šé¢è®¾ç½®äº†æˆªæ®µçš„é€‰é¡¹`truncation=True`ï¼Œè¿™ä¸ªå‚æ•°çš„å¯èƒ½çš„é€‰é¡¹æœ‰ï¼š\n",
    "\n",
    "* `True`æˆ–è€…`longest_first`ï¼šæˆªæ–­åˆ°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œå¦‚æœå¦‚æœæ²¡æœ‰æä¾›`max_length`å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹æ¥å—çš„æœ€å¤§é•¿åº¦ï¼ˆ`model_max_length`ï¼‰ã€‚å¦‚æœä¼ å…¥çš„æ˜¯ä¸€ä¸ªå¥å­å¯¹ï¼Œé‚£ä¹ˆå®ƒå°†ä»æœ€é•¿çš„é‚£ä¸ªå¥å­ä¸­åˆ é™¤å­—ç¬¦ï¼Œç›´åˆ°é•¿åº¦æ»¡è¶³ä¸ºæ­¢ã€‚å¦‚æœåœ¨åˆ é™¤çš„è¿‡ç¨‹ä¸­ï¼Œä¼šä¸æ–­çš„æ£€æŸ¥å‰©ä½™çš„ä¸¤ä¸ªå¥å­çš„é•¿åº¦ï¼Œé€‰é¡¹æœ€é•¿çš„é‚£ä¸ªå¥å­æ¥åˆ é™¤tokenã€‚\n",
    "* `only_second`: å¯¹äºå•ä¸ªå¥å­æˆªæ–­çš„è¡Œä¸ºä¸`True`ä¸€è‡´ã€‚å¦‚æœæä¾›çš„æ˜¯ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹æˆå¯¹çš„åºåˆ—ï¼‰ï¼Œè¿™åªä¼šæˆªæ–­ä¸€å¯¹åºåˆ—çš„ç¬¬äºŒå¥ï¼Œå¦‚æœç¬¬äºŒä¸ªå¥å­ä¸å¤Ÿæˆªå–ï¼Œåˆ™æŠ¥é”™ã€‚\n",
    "* `only_first`: å¯¹äºå•ä¸ªå¥å­æˆªæ–­çš„è¡Œä¸ºä¸`True`ä¸€è‡´ã€‚å¦‚æœæä¾›çš„æ˜¯ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹æˆå¯¹çš„åºåˆ—ï¼‰ï¼Œè¿™åªä¼šæˆªæ–­ä¸€å¯¹åºåˆ—çš„ç¬¬ä¸€å¥ï¼Œå¦‚æœç¬¬ä¸€ä¸ªå¥å­ä¸å¤Ÿæˆªå–ï¼Œåˆ™æŠ¥é”™ã€‚\n",
    "* `False`æˆ–è€…`do_not_truncate`ï¼šä¸è¿›è¡Œæˆªæ–­ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. åˆ†è¯ï¼štokenizeï¼šå°†å¥å­è½¬æ¢ä¸ºtokenï¼ˆword pieceï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today', 'is', 'not', 'so', 'bad']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ä¸æ”¯æŒbatchè°ƒç”¨\n",
    "tokenizer.tokenize(test_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[UNK]', 'ï¼Œ', 'ä¸­', 'å›½', 'ï¼Œ', 'bert', '[UNK]', 'ä¸­', 'æ–‡', 'çš„', '[UNK]', '[UNK]', '[UNK]', 'æœ‰', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "# æˆ‘ä»¬ä½¿ç”¨çš„Bertæ¨¡å‹å¯¹ä¸­æ–‡æ”¯æŒæœ‰é™ï¼Œä¸åœ¨vocabä¸­çš„ä¸­æ–‡ä¼šè¢«è½¬æ¢ä¸º [UNK]\n",
    "print(tokenizer.tokenize(\"ä½ å¥½ï¼Œä¸­å›½ï¼ŒBertå¯¹ä¸­æ–‡çš„æ”¯æŒå¾ˆæœ‰é™\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '-', 'cat', '!', ',', 'hugging', '##face', ',', '123', '##45', '##6']\n"
     ]
    }
   ],
   "source": [
    "# å¯¹äºä¸€äº›ç”Ÿåƒ»ï¼Œé”™è¯¯çš„wordï¼Œä¼šè¿›è¡Œæ‹†ä¸ºwordpiece\n",
    "print(tokenizer.tokenize(\"hello-cat!, huggingFace, 123456\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '-', 'cat', '!', ',', 'hugging', '##face', ',', '123', '##45', '##6']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"hello-cat!, huggingFace, 123456\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. tokenå’Œidçš„ç›¸äº’è½¬æ¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2651, 2003, 2025, 2061, 2919]\n",
      "['today', 'is', 'not', 'so', 'bad']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(test_examples[0])\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)\n",
    "print(tokenizer.convert_ids_to_tokens(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. `encode`\n",
    "\n",
    "encodeæ¥å£åªèƒ½è¿”å› token idsï¼Œè€Œä¸”ä¸æ”¯æŒbatchè°ƒç”¨ã€‚ç­‰ä»·äºï¼š`convert_token_to_id(tokenize(text))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2651, 2003, 2025, 2061, 2919, 102, 2009, 2003, 2061, 2919, 102]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\n",
    "    test_examples[0],\n",
    "    test_examples[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'today', 'is', 'not', 'so', 'bad', '[SEP]', 'it', 'is', 'so', 'bad', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tokenizer.convert_ids_to_tokens(\n",
    "        tokenizer.encode(test_examples[0], test_examples[1])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. `encode_plus`\n",
    "\n",
    "è¯¥æ¥å£å·²ç»deprecatedï¼Œå®Œå…¨è¢«`__call__`æ›¿æ¢ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. decode\n",
    "\n",
    "decodeæ˜¯encodeçš„é€†è¿ç®—ï¼šå°†id list è½¬åŒ–ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] today is not so bad [SEP] it is so bad [SEP]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(test_examples[0], test_examples[1])\n",
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fast Tokenizer / Slow Tokenizer\n",
    "\n",
    "Fast Tokenizer æ˜¯åŸºäºrustæ¥å®ç°çš„ï¼Œé€Ÿåº¦å¿«ï¼›è€ŒSlow tokenizeræ˜¯åŸºäºpythonå®ç°ï¼Œé€Ÿåº¦æ…¢ï¼›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "slow_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_long_texts = [\n",
    "    \"A tokenizer is in charge of preparing the inputs for a model. The library contains tokenizers for all the models. Most of the tokenizers are available in two flavors: a full python implementation and a â€œFastâ€ implementation based on the Rust library ğŸ¤— Tokenizers. The â€œFastâ€ implementations allows:\",\n",
    "    \"The base classes PreTrainedTokenizer and PreTrainedTokenizerFast implement the common methods for encoding string inputs in model inputs (see below) and instantiating/saving python and â€œFastâ€ tokenizers either from a local file or directory or from a pretrained tokenizer provided by the library (downloaded from HuggingFaceâ€™s AWS S3 repository). They both rely on PreTrainedTokenizerBase that contains the common methods, and SpecialTokensMixin.\",\n",
    "    \"BatchEncoding holds the output of the PreTrainedTokenizerBaseâ€™s encoding methods (__call__, encode_plus and batch_encode_plus) and is derived from a Python dictionary. When the tokenizer is a pure python tokenizer, this class behaves just like a standard python dictionary and holds the various model inputs computed by these methods (input_ids, attention_maskâ€¦). When the tokenizer is a â€œFastâ€ tokenizer (i.e., backed by HuggingFace tokenizers library), this class provides in addition several advanced alignment methods which can be used to map between the original string (character and words) and the token space (e.g., getting the index of the token comprising a given character or the span of characters corresponding to a given token).\",\n",
    "    \"Handle all the shared methods for tokenization and special tokens as well as methods downloading/caching/loading pretrained tokenizers as well as adding tokens to the vocabulary.\",\n",
    "]\n",
    "# bathc size = 4 * 8\n",
    "batch_long_texts = batch_long_texts * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.94 ms Â± 9.04 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "fast_tokenizer(batch_long_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 ms Â± 446 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "slow_tokenizer(batch_long_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»ä¸Šé¢çš„å¯¹æ¯”ï¼Œå¯ä»¥çœ‹å‡ºåœ¨æ‰¹é‡æ¨¡å‹å¼ä¸‹ï¼ŒFastTokenizeræ˜¯SlowTokenizerçš„20å€ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. `offset_mapping`å’Œ`word_ids`\n",
    "FastTokenizeræœ‰ä¸€äº›ç‰¹æ®Šçš„è¿”å›å€¼\n",
    "\n",
    "* offset_mappingï¼šæ ‡è®°äº†æ¯ä¸€ä¸ªtokenåœ¨åŸè¾“å‡ºsträ¸­å­—ç¬¦çº§åˆ«çš„ç´¢å¼•ä½ç½®\n",
    "* word_idsï¼šæ ‡è®°äº†æ¯ä¸ªtokenå¯¹åº”åŸè¾“å‡ºä¸­wordçš„ç´¢å¼•\n",
    "\n",
    "è¿™ä¸ªå¯¹äºNERæˆ–QAæ¥è¯´æ¯”è¾ƒé‡è¦ã€‚\n",
    "\n",
    "<div align=\"left\">\n",
    "  <img src=\"./assets/tokenizer.drawio.svg\" width=\"660\"/> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1999, 1996, 2502, 2502, 2088, 1010, 1045, 2031, 1037, 2502, 3959, 6562, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 2), (3, 6), (7, 10), (11, 14), (15, 20), (20, 21), (22, 23), (24, 28), (29, 30), (31, 34), (35, 40), (40, 44), (0, 0)]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = fast_tokenizer(\n",
    "    \"In the big big world, I have a big dreamming\", return_offsets_mapping=True\n",
    ")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 2), (3, 6), (7, 10), (11, 14), (15, 20), (20, 21), (22, 23), (24, 28), (29, 30), (31, 34), (35, 40), (40, 44), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "print(inputs.offset_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, None]\n"
     ]
    }
   ],
   "source": [
    "print(inputs.word_ids())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
