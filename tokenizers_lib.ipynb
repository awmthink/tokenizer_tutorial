{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizersåº“çš„ä½¿ç”¨ä»‹ç»\n",
    "\n",
    "tokenizersåº“æ˜¯HuggingFaceä½¿ç”¨Rustçš„å®ç°çš„ä¸€ä¸ªåŒæ—¶é’ˆå¯¹ç ”ç©¶ä¸ç”Ÿäº§çš„æ–‡æœ¬tokenizationçš„åº“ã€‚åŒæ—¶å®ƒä¹Ÿæ˜¯transformersåº“ä¸­FastTokenizerèƒŒåçš„å®ç°ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ„å»ºTokenizerç±»ä¸Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BpeTrainer, UnigramTrainer, WordLevelTrainer, WordPieceTrainer\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# å†™å…¥ç‰¹æ®Šæ ‡è®°åˆ—è¡¨çš„é¡ºåºå¾ˆé‡è¦ï¼šåœ¨è¿™é‡Œï¼Œ \"[UNK]\" å°†è·å¾— ID 0ï¼Œ \"[CLS]\" å°†è·å¾— ID 1ï¼Œä»¥æ­¤ç±»æ¨ã€‚\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ·»åŠ Pre-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨PreTokenizerå¯ä»¥ç¡®ä¿æˆ‘ä»¬æœ€ç»ˆä¸ä¼šç”Ÿæˆ wordä¸wordçº§åˆ«çš„åˆå¹¶\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®­ç»ƒ\n",
    "\n",
    "é¦–å…ˆæˆ‘ä»¬éœ€è¦å…ˆä¸‹è½½ [wikitext-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) æ•°æ®é›†ï¼Œå¹¶ä¸”è§£å‹åˆ°`data`ç›®å½•ä¸‹ã€‚\n",
    "\n",
    "```bash\n",
    "mkdir data && cd data\n",
    "wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
    "unzip wikitext-103-raw-v1.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    f\"./data/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]\n",
    "]\n",
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¿å­˜è®­ç»ƒå¥½çš„Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"./data/wikitext-103-raw/tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä»æ–‡ä»¶åŠ è½½Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"./data/wikitext-103-raw/tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode(\"Hello, y'all! How are you ğŸ˜ ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=11, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'y', \"'\", 'all', '!', 'How', 'are', 'you', '[UNK]', '?']\n"
     ]
    }
   ],
   "source": [
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]\n"
     ]
    }
   ],
   "source": [
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13), (14, 17), (18, 21), (22, 25), (26, 27), (28, 29)]\n"
     ]
    }
   ],
   "source": [
    "print(output.offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ·»åŠ åå¤„ç†\n",
    "\n",
    "æˆ‘ä»¬æŒ‡å®šå¥å­å¯¹æ¨¡æ¿ï¼Œå…¶å½¢å¼åº”ä¸º \"[CLS] $A [SEP] $B [SEP]\" ï¼Œå…¶ä¸­ $A ä»£è¡¨ç¬¬ä¸€å¥ï¼Œ $B ä»£è¡¨ç¬¬äºŒå¥ã€‚æ¨¡æ¿ä¸­æ·»åŠ çš„ :1 ä»£è¡¨æˆ‘ä»¬å¸Œæœ›è¾“å…¥çš„æ¯ä¸€éƒ¨åˆ†çš„ type IDs ï¼šé»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰å†…å®¹çš„ éƒ½ä¸º 0ï¼ˆè¿™ä¹Ÿæ˜¯æˆ‘ä»¬æ²¡æœ‰ $A:0 çš„åŸå› ï¼‰ï¼Œè¿™é‡Œæˆ‘ä»¬å°†ç¬¬äºŒå¥çš„æ ‡è®°å’Œæœ€åä¸€ä¸ª \"[SEP]\" æ ‡è®°è®¾ç½®ä¸º 1ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å†æ¥æ£€æŸ¥ç¼–ç ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Hello', ',', 'y', \"'\", 'all', '!', 'How', 'are', 'you', '[UNK]', '?', ',', 'ab', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(\"Hello, y'all! How are you ğŸ˜ ?, ab\")\n",
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'is', 'sentence', '1', '[SEP]', 'this', 'is', 'sentence', '2', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(\"this is sentence 1\", \"this is sentence 2\")\n",
    "print(output.tokens)\n",
    "print(output.type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ‰¹é‡Encode - `encode_batch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.encode_batch(\n",
    "    [\n",
    "        [\"Hello, y'all!\", \"How are you ğŸ˜ ?\"],\n",
    "        [\n",
    "            \"Hello to you too!\",\n",
    "            \"I'm fine, thank you!\",\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'How', 'are', 'you', '[UNK]', '?', '[SEP]', '[PAD]']\n",
      "[1, 1, 1, 1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.enable_padding(pad_id=3, pad_token=\"[PAD]\")\n",
    "output = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you ğŸ˜ ?\"])\n",
    "print(output[1].tokens)\n",
    "print(output[1].attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents\n",
    "\n",
    "normalizer = normalizers.Sequence([NFD(), StripAccents()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello how are u?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.normalize_str(\"HÃ©llÃ² hÃ´w are Ã¼?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers.pre_tokenizers import Digits, Whitespace\n",
    "\n",
    "pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Call', (0, 4)), ('9', (5, 6)), ('1', (6, 7)), ('1', (7, 8))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer.pre_tokenize_str(\"Call 911\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BertTokenizer from Scatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, normalizers\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_uncased_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "bert_uncased_tokenizer.normalizer = normalizers.Sequence(\n",
    "    [NFD(), Lowercase(), StripAccents()]\n",
    ")\n",
    "bert_uncased_tokenizer.pre_tokenizer = Whitespace()\n",
    "bert_uncased_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=30522, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "files = [\n",
    "    f\"./data/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]\n",
    "]\n",
    "bert_uncased_tokenizer.train(files, trainer)\n",
    "bert_uncased_tokenizer.save(\"./data/wikitext-103-raw/bert-wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'welcome', 'to', 'the', '[UNK]', 'tok', '##eni', '##zer', '##s', 'library', '.', '[SEP]']\n",
      "[1, 18263, 7128, 7108, 0, 22453, 27107, 12800, 4073, 11046, 18, 2]\n"
     ]
    }
   ],
   "source": [
    "output = bert_uncased_tokenizer.encode(\"Welcome to the ğŸ¤— Tokenizers library.\")\n",
    "print(output.tokens)\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'welcome to the tok ##eni ##zer ##s library .'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_uncased_tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'welcome to the tokenizers library.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import decoders\n",
    "\n",
    "bert_uncased_tokenizer.decoder = decoders.WordPiece()\n",
    "bert_uncased_tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä»ä¸€ä¸ªæ•°æ®è¿­ä»£å™¨æ¥è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    Tokenizer,\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    trainers,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.Unigram())\n",
    "tokenizer.normalizer = normalizers.NFKC()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "trainer = trainers.UnigramTrainer(\n",
    "    vocab_size=20000,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    "    special_tokens=[\"<PAD>\", \"<BOS>\", \"<EOS>\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First few lines of the \"Zen of Python\" https://www.python.org/dev/peps/pep-0020/\n",
    "data = [\n",
    "    \"Beautiful is better than ugly.\"\n",
    "    \"Explicit is better than implicit.\"\n",
    "    \"Simple is better than complex.\"\n",
    "    \"Complex is better than complicated.\"\n",
    "    \"Flat is better than nested.\"\n",
    "    \"Sparse is better than dense.\"\n",
    "    \"Readability counts.\"\n",
    "]\n",
    "tokenizer.train_from_iterator(data, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "datasets.logging.set_verbosity_error()\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "    \"wikitext\", \"wikitext-103-raw-v1\", split=\"train+test+validation\"\n",
    ")\n",
    "\n",
    "\n",
    "def batch_iterator(batch_size=1000):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"text\"]\n",
    "\n",
    "\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åœ¨ä¸€ä¸ªæ—§çš„Tokenizerä¸Šè®­ç»ƒ\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨`transfomers`åº“ä¸­çš„`AutoTokenizer.train_new_from_iterator()`æ¥åœ¨ä¸€ä¸ªå·²æœ‰çš„Tokenizerä¸Šç”¨æ–°æ•°æ®è¿›è¡Œè®­ç»ƒã€‚æ–°çš„Tokenizeré™¤äº†å†…éƒ¨çš„åˆ†è¯å™¨æ¨¡å‹çš„ç›¸å…³è¯è¡¨ç­‰ä¸åŒå¤–ï¼Œå…¶ä½™å‡ä¸åŸåˆ†è¯å™¨ä¸€è‡´ã€‚\n",
    "\n",
    "ä¸‹é¢æˆ‘ä»¬å°†æ¼”ç¤ºåœ¨`gpt-2`çš„åˆ†è¯å™¨ä¸Šï¼Œä¸ºPythonä»£ç æ¥è®­ç»ƒä¸€ä¸ªä¸“é—¨çš„åˆ†è¯å™¨ã€‚CodeSearchNet æ•°æ®é›†æ˜¯ä¸º CodeSearchNet æŒ‘æˆ˜èµ›åˆ›å»ºçš„ï¼ŒåŒ…å«æ¥è‡ª GitHub ä¸Šå¤šä¸ªç¼–ç¨‹è¯­è¨€å¼€æºåº“çš„æ•°ç™¾ä¸‡ä¸ªå‡½æ•°ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬å°†åŠ è½½è¯¥æ•°æ®é›†çš„ Python éƒ¨åˆ†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_dataset = load_dataset(\"code_search_net\", \"python\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ•°æ®é›†ä¸­åŒ…æ‹¬äº†è®¸å¤šçš„å­—æ®µï¼Œæˆ‘ä»¬è¿™é‡Œé¢ä½¿ç”¨`whole_func_string`å­—æ®µï¼Œå®ƒåŒ…æ‹¬äº†å‡½æ•°çš„ä»£ç ä»¥åŠå‡½æ•°çš„æ³¨é‡Šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 412178\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_dataset[i : i + 1024][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_dataset), 1024)\n",
    "    )\n",
    "\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ä add', '_', 'n', 'umbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä \"\"\"', 'Add', 'Ä the', 'Ä two', 'Ä numbers', 'Ä `', 'a', '`', 'Ä and', 'Ä `', 'b', '`', '.\"', '\"\"', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']\n"
     ]
    }
   ],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™ä¸ªTokenizeræœ‰ä¸€äº›ç‰¹æ®Šç¬¦å·ï¼Œå¦‚ Ä  å’Œ ÄŠ ï¼Œåˆ†åˆ«è¡¨ç¤ºç©ºæ ¼å’Œæ¢è¡Œã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¿™æ ·åšçš„æ•ˆç‡å¹¶ä¸é«˜ï¼šTokenizerä¼šä¸ºæ¯ä¸ªç©ºæ ¼è¿”å›å•ç‹¬çš„æ ‡è®°ç¬¦å·ï¼Œè€Œå®ƒæœ¬å¯ä»¥å°†ç¼©è¿›çº§åˆ«åˆ†ç»„ï¼ˆå› ä¸ºåœ¨ä»£ç ä¸­ï¼Œ4 ä¸ªæˆ– 8 ä¸ªç©ºæ ¼æ˜¯å¾ˆå¸¸è§çš„ï¼‰ã€‚ç”±äºä¸ä¹ æƒ¯çœ‹åˆ°å¸¦æœ‰ _ å­—ç¬¦çš„å•è¯ï¼Œå®ƒå¯¹å‡½æ•°åçš„åˆ†å‰²ä¹Ÿæœ‰ç‚¹å¥‡æ€ªã€‚\n",
    "\n",
    "è®©æˆ‘ä»¬æ¥è®­ç»ƒä¸€ä¸ªæ–°çš„æ ‡è®°ç¬¦ï¼Œçœ‹çœ‹å®ƒæ˜¯å¦èƒ½è§£å†³è¿™äº›é—®é¢˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ä add', '_', 'numbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠÄ Ä Ä ', 'Ä \"\"\"', 'Add', 'Ä the', 'Ä two', 'Ä numbers', 'Ä `', 'a', '`', 'Ä and', 'Ä `', 'b', '`.\"\"\"', 'ÄŠÄ Ä Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å†æ¬¡çœ‹åˆ°äº†è¡¨ç¤ºç©ºæ ¼å’Œæ¢è¡Œçš„ç‰¹æ®Šç¬¦å· Ä  å’Œ ÄŠ ï¼Œä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„tokenizerå­¦ä¹ äº†ä¸€äº›å¯¹ Python å‡½æ•°è¯­æ–™åº“æ¥è¯´éå¸¸ç‰¹æ®Šçš„æ ‡è®°ï¼šä¾‹å¦‚ï¼Œæœ‰ä¸€ä¸ª `ÄŠÄ Ä Ä ` æ ‡è®°è¡¨ç¤ºç¼©è¿›ï¼Œè¿˜æœ‰ä¸€ä¸ª `Ä \"\"\"` æ ‡è®°è¡¨ç¤ºæ–‡æ¡£å­—ç¬¦ä¸²å¼€å§‹çš„ä¸‰ä¸ªå¼•å·ã€‚æ ‡è®°ç¬¦è¿˜èƒ½æ­£ç¡®åˆ†å‰² `_` ä¸Šçš„å‡½æ•°åã€‚è¿™æ˜¯ä¸€ä¸ªç›¸å½“ç´§å‡‘çš„è¡¨ç¤ºæ³•ï¼›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 'Ä Linear', 'Layer', '():', 'ÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'init', '__(', 'self', ',', 'Ä input', '_', 'size', ',', 'Ä output', '_', 'size', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'weight', 'Ä =', 'Ä torch', '.', 'randn', '(', 'input', '_', 'size', ',', 'Ä output', '_', 'size', ')', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'bias', 'Ä =', 'Ä torch', '.', 'zeros', '(', 'output', '_', 'size', ')', 'ÄŠÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'call', '__(', 'self', ',', 'Ä x', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä return', 'Ä x', 'Ä @', 'Ä self', '.', 'weights', 'Ä +', 'Ä self', '.', 'bias', 'ÄŠÄ Ä Ä Ä ']\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"class LinearLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weight = torch.randn(input_size, output_size)\n",
    "        self.bias = torch.zeros(output_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "    \"\"\"\n",
    "print(tokenizer.tokenize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯¹äºä¸Šé¢è¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°åŒç¼©è¿›çš„æ ‡è®°ï¼š`ÄŠÄ Ä Ä Ä Ä Ä Ä `ï¼Œä»¥åŠåƒ`class`ï¼Œ`init`ï¼Œ`call`ï¼Œ`self`è¿™æ ·çš„ç‰¹ç pythonå…³é”®å­—ä¹Ÿä¼šè¢«æ ‡è®°ä¸ºä¸€ä¸ªtokenï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œé™¤äº†å¯¹ _ å’Œ . è¿›è¡Œæ‹†åˆ†å¤–ï¼Œæ ‡è®°ç¬¦è¿˜èƒ½æ­£ç¡®æ‹†åˆ†é©¼å³°å­—æ¯çš„åç§°ï¼š LinearLayer è¢«æ ‡è®°ä¸º [\"Ä Linear\", \"Layer\"] ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizersåº“çš„ä¸»è¦ç»„ä»¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"./assets/tokenizers.png\" width=\"600\"/> </div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
